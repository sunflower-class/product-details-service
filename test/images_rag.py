import pandas as pd
import chromadb
from sentence_transformers import SentenceTransformer
from PIL import Image
import os
import base64
import openai
import shutil
from dotenv import load_dotenv

def generate_caption_for_image(client, image_path: str) -> str:
    """GPT-4o Vision ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"   - ğŸ–¼ï¸ '{image_path}' ìº¡ì…˜ ìƒì„± ì¤‘...")
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "user", "content": [
                    {"type": "text", "text": "Describe this image in detail..."},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]}
            ], max_tokens=100
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"    - ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
        return ""

def build_rag_index(model, client_openai, client_chroma, csv_path: str, image_directory: str):
    """(í•˜ì´ë¸Œë¦¬ë“œìš©) ì´ë¯¸ì§€ ë²¡í„°ì™€ í…ìŠ¤íŠ¸ ë²¡í„°ë¥¼ ê°ê°ì˜ ì»¬ë ‰ì…˜ì— ì €ì¥í•©ë‹ˆë‹¤."""
    # (í´ë” ì‚­ì œ ë¡œì§ì„ main ë¸”ë¡ìœ¼ë¡œ ì´ë™ì‹œí‚´)
    
    collection_image = client_chroma.get_or_create_collection(name="image_vectors", metadata={"hnsw:space": "cosine"})
    collection_text = client_chroma.get_or_create_collection(name="text_vectors", metadata={"hnsw:space": "cosine"})
    
    print(f"\n--- ğŸ”„ '{csv_path}' íŒŒì¼ ê¸°ë°˜ ì¸ë±ì‹± ì‹œì‘ ---")
    df = pd.read_csv(csv_path)

    for index, row in df.iterrows():
        item_id = f"item_{index}"
        image_name = row['ì´ë¯¸ì§€ íŒŒì¼ëª…']
        image_path = os.path.join(image_directory, image_name)
        if not os.path.exists(image_path):
            print(f"   - âš ï¸ ê²½ê³ : '{image_path}' íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        image_embedding = model.encode(Image.open(image_path))
        caption = generate_caption_for_image(client_openai, image_path)
        tags = str(row['ì¶”ì¶œëœ í‚¤ì›Œë“œ'])
        text_to_embed = f"Tags: {tags}. Caption: {caption}"
        text_embedding = model.encode(text_to_embed)
        metadata = {"ì´ë¦„": str(image_name), "íƒœê·¸": tags, "ê²½ë¡œ": image_path, "ìº¡ì…˜": caption, "url": row['cloud_url']}
        
        collection_image.add(embeddings=[image_embedding.tolist()], metadatas=[metadata], ids=[item_id])
        collection_text.add(embeddings=[text_embedding.tolist()], metadatas=[metadata], ids=[item_id])
        
        print(f"   - âœ… '{item_id}' ({image_name}) ì¸ë±ì‹± ì™„ë£Œ")
    print("\n--- âœ¨ ì¸ë±ì‹± ì‘ì—… ì™„ë£Œ ---")


if __name__ == "__main__":
    # 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # 2. ê²½ë¡œ ì„¤ì •
    DB_PATH = "./chroma_db"
    CSV_FILE_PATH = "/Users/seoyeong/product-details-service/test/image_keywords_cloud.csv" 
    IMAGE_DIRECTORY = "/Users/seoyeong/product-details-service/test/generated_images"

    # 3. DB í´ë” ì‚­ì œ (ê°€ì¥ ë¨¼ì € ì‹¤í–‰)
    if os.path.exists(DB_PATH):
        print(f"âš ï¸ ê¸°ì¡´ '{DB_PATH}' í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
        shutil.rmtree(DB_PATH)

    # 4. ëª¨ë¸ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í´ë” ì‚­ì œ í›„ ì‹¤í–‰)
    print("ğŸ¤– ëª¨ë¸ ë° í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    model = SentenceTransformer('clip-ViT-B-32')
    client_openai = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    client_chroma = chromadb.PersistentClient(path=DB_PATH) # ìˆ˜ì •ëœ DB_PATH ì‚¬ìš©
    print("âœ… ì´ˆê¸°í™” ì™„ë£Œ.")

    # 5. RAG ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤í–‰
    print("--- RAG ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    build_rag_index(model, client_openai, client_chroma, CSV_FILE_PATH, IMAGE_DIRECTORY)
    print("--- RAG ì¸ë±ìŠ¤ êµ¬ì¶•ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤ ---")